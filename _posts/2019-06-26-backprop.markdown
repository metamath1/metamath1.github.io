---
layout: post
title:  "역전파 알고리즘 완전정복 A Step by Step Backpropagation"
date:   2019-06-26
---

딥러닝을 공부하다보면 가장 먼저 만나는 장벽이 역전파 알고리즘을 이해하는 과정일 것이다. 역전파 알고리즘은 손실 함수cost function, loss function를 무수히 많은 변수들로 미분하는 과정인데 이때 변수의 인덱스, 신경망 층에 대한 인덱스 등 복잡한 인덱스가 등장한다. 단계적으로 특정 규칙을 따라 진행되는 알고리즘은 전체 전체 과정을 한눈에 파악하지 못하게 만드는 나쁜 특징이기도 하다. 

이런 여러 여러운 점에도 불구하고 역전파 알고리즘을 다변수 함수 미분의 연쇄법칙으로 비교적 명쾌하게 설명할 수 있다. 원리적으로 설명된 좋은 자료들이 많이 존재하지만 전체적인 절차만 이야기할 뿐 복잡한 행렬 미분 따위는 자세히 풀어쓰지 않고 있다. 미분 과정과 구체적인 실행을 자세히 풀어적은 한국어 문서는 거의 찾아 보기 힘들다.

이런 이유로 인해 본 글은 아주 작은 장난감 네트워크를 예제로 이용하여 네트워크 가중치weight와 편향bias에 대한 손실함수의 미분이 행렬-벡터의 미분 공식대로 계산되는지 한줄 한줄 따라하기 식으로 풀어 적는 것을 목적으로 하고 있다. 각 단계에 해당하는 그림을 한장 제시하고 그 그림에서 일어나는 수식전개를 자세히 설명하였다. 또한 수식을 전개하는 것에 그치지 않고 numpy로 유도된 수식을 직접 계산을 해보고 그 결과를 pytorch.autograd.grad함수를 통해 미분한 결과와 비교 확인하는 것까지 진행해보고자 한다.

본 글을 통해 딥러닝에 입문한 많은 국내 학습자들이 역전파 알고리즘을 이해하기 위해 낭비하는 시간을 조금이라도 줄일 수 있으면 하는 바람이다.

![]({{ "/assets/backprop.png" | absolute_url }})
 
전체 글은 jupyter notebook으로 작성되어 있어서 아래 링크를 통해 nbviewer로 공유한다.

[A Step by Step Backpropagation][backprop]

[backprop]: https://nbviewer.jupyter.org/github/metamath1/ml-simple-works/blob/master/BP/bp.ipynb
